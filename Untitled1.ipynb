{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from environments import EnvSetTarget\n",
    "from environments import EnvChaseTarget\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = EnvSetTarget(6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 0]), 6, False, None)\n",
      "(array([2, 1]), 0, False, None)\n",
      "(array([3, 2]), 0, True, None)\n",
      "(array([4, 2]), 6, True, None)\n",
      "lives_lost needs to be within  0  and  2\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0ab1edce97fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/navin/15dc2a57-8c94-4a73-8054-ca78c9ac266f/Books/COMP 767/Project/environments.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_steps_elapsed\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlives_lost\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all_lives_lost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__time_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_steps_elapsed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlives_lost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/navin/15dc2a57-8c94-4a73-8054-ca78c9ac266f/Books/COMP 767/Project/environments.py\u001b[0m in \u001b[0;36mlives_lost\u001b[0;34m(self, lives_lost)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lives_lost needs to be within \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_lives_lost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" and \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_lives_lost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(env.step(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = EnvChaseTarget(max_time_steps=6,max_lives_lost=2,target=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 6]), 0, False, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def double_Q_learning(env,num_episodes,Q1,Q2,alpha,epsilon,gamma,lamb):\n",
    "    \"\"\"\n",
    "    :param : env\n",
    "    :param : num_episodes \n",
    "    :param : Q1 - 2D array of values for state action pairs\n",
    "    :param : Q2 - 2D array of values for state action pairs\n",
    "    :param : alpha - learning rate\n",
    "    :param : epsilon - explore-exploit trade-off parameter\n",
    "    :param : gamma - discount factor\n",
    "    :param : lamb - eligibility trace decay factor\n",
    "    \"\"\"\n",
    "\n",
    "    l = [] #this list will store the scores achieved in all episodes\n",
    "    for _ in range(num_episodes):\n",
    "        total_score = 0#at the start of every new episode set total score to 0\n",
    "        eligibility=np.zeros(Q1.shape)#define eligibility\n",
    "        observation = env.reset()# reset the environment to start\n",
    "        r_list = []\n",
    "        while(True):\n",
    "            R = np.random.randint(1,3)#random varibale to decide if to use Q1 or Q2\n",
    "            #input(\"R=\"+str(R))\n",
    "            if R==1: #if random variable R = 1 then choose Q1 to take action\n",
    "                Q=Q1\n",
    "                criticQ=Q2\n",
    "            else: \n",
    "                Q=Q2 #if random variable R = 2 then choose Q2 to take action\n",
    "                criticQ=Q1\n",
    "\n",
    "            Q_old = deepcopy(Q)#make a copy of Q\n",
    "            x = np.random.binomial(n=1,size=1,p=1-epsilon)#bernoulli random variable with p=1-epsilon\n",
    "            action = -1\n",
    "            #input(\"x is\" +str(x))\n",
    "            #input(Q)\n",
    "            #input(criticQ)\n",
    "            if x[0] == 1:#exploitation\n",
    "                #choose action with highest avg value\n",
    "                action = np.argmax(0.5*criticQ[observation[0],observation[1],:]+0.5*Q[observation[0],observation[1],:])\n",
    "            else:#exploration\n",
    "                action = np.random.randint(low=0,high=4)\n",
    "            #input(\"action is \" + str(action))\n",
    "            #before taking an action store the current state\n",
    "            old_observation = deepcopy(observation)\n",
    "            #pull up the eligibility of the current state action by 1.\n",
    "            eligibility[old_observation[0],old_observation[1],action] += 1\n",
    "            #input(eligibility)\n",
    "            #take an action\n",
    "            observation, reward, done, info = env.step(action)#take action\n",
    "            \n",
    "            total_score += max(reward,0)#update the score\n",
    "            r_list.append(reward)\n",
    "            \"\"\"\n",
    "            Q-learning update step\n",
    "            \"\"\"\n",
    "            #Q[old_observation[0], action, old_observation[1]] = \\\n",
    "            #    Q[old_observation[0], action, old_observation[1]] + alpha * (\n",
    "            #        reward + gamma*criticQ[observation[0],\n",
    "            #                               np.argmax(Q[observation[0], :, observation[1]]),observation[1]] \n",
    "            #        - Q[old_observation[0], action, old_observation[1]])\n",
    "            \n",
    "            \n",
    "            #calculate delta by using double Q update \n",
    "            delta = reward + gamma*criticQ[observation[0],\n",
    "                    observation[1],np.argmax(Q[observation[0], observation[1], :])] \n",
    "            - Q[old_observation[0], old_observation[1], action]\n",
    "            #input(\"delta\")\n",
    "            #input(delta)\n",
    "            #update the Q \n",
    "            Q += alpha * delta * eligibility\n",
    "            #input(\"Q\")\n",
    "            #input(Q)\n",
    "            #decay the eligibility\n",
    "            eligibility *= lamb\n",
    "\n",
    "            if done:#if the episode has terminated\n",
    "                l.append(total_score)#append the score in the episode to list\n",
    "                break\n",
    "\n",
    "    return np.array(l)#return the list of scores achieved in all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dup_double_Q_learning(env,num_episodes,Q1,Q2,alpha,epsilon,gamma,lamb):\n",
    "    \"\"\"\n",
    "    :param : env\n",
    "    :param : num_episodes \n",
    "    :param : Q1 - 2D array of values for state action pairs\n",
    "    :param : Q2 - 2D array of values for state action pairs\n",
    "    :param : alpha - learning rate\n",
    "    :param : epsilon - explore-exploit trade-off parameter\n",
    "    :param : gamma - discount factor\n",
    "    :param : lamb - eligibility trace decay factor\n",
    "    \"\"\"\n",
    "\n",
    "    l = [] #this list will store the scores achieved in all episodes\n",
    "    for _ in range(num_episodes):\n",
    "        total_score = 0#at the start of every new episode set total score to 0\n",
    "        eligibility1=np.zeros(Q1.shape)#define eligibility\n",
    "        eligibility2=np.zeros(Q1.shape)#define eligibility\n",
    "        eligibility = np.zeros(Q1.shape)\n",
    "        observation = env.reset()# reset the environment to start\n",
    "        r_list = []\n",
    "        while(True):\n",
    "            R = np.random.randint(1,3)#random varibale to decide if to use Q1 or Q2\n",
    "            #input(\"R=\"+str(R))\n",
    "            if R==1: #if random variable R = 1 then choose Q1 to take action\n",
    "                Q=Q1\n",
    "                criticQ=Q2\n",
    "            else: \n",
    "                Q=Q2 #if random variable R = 2 then choose Q2 to take action\n",
    "                criticQ=Q1\n",
    "\n",
    "            Q_old = deepcopy(Q)#make a copy of Q\n",
    "            x = np.random.binomial(n=1,size=1,p=1-epsilon)#bernoulli random variable with p=1-epsilon\n",
    "            action = -1\n",
    "            #input(\"x is\" +str(x))\n",
    "            #input(Q)\n",
    "            #input(criticQ)\n",
    "            if x[0] == 1:#exploitation\n",
    "                #choose action with highest avg value\n",
    "                #action = np.argmax(0.5*criticQ[observation[0],observation[1],:]+0.5*Q[observation[0],observation[1],:])\n",
    "                action = np.argmax(0.5*criticQ[observation[0],observation[1],:]+0.5*Q[observation[0],observation[1],:])\n",
    "            else:#exploration\n",
    "                action = np.random.randint(low=0,high=4)\n",
    "            #input(\"action is \" + str(action))\n",
    "            #before taking an action store the current state\n",
    "            old_observation = deepcopy(observation)\n",
    "            #pull up the eligibility of the current state action by 1.\n",
    "            #if R==1:\n",
    "            #    eligibility1[old_observation[0],action,old_observation[1]] += 1\n",
    "            #    eligibility = eligibility1\n",
    "            #else:\n",
    "                \n",
    "            #    eligibility2[old_observation[0],action,old_observation[1]] += 1\n",
    "            #    eligibility = eligibility2\n",
    "            eligibility[old_observation[0],old_observation[1],action] += 1\n",
    "            #input(eligibility)\n",
    "            #take an action\n",
    "            observation, reward, done, info = env.step(action)#take action\n",
    "            \n",
    "            total_score += max(reward,0)#update the score\n",
    "            r_list.append(reward)\n",
    "            \"\"\"\n",
    "            Q-learning update step\n",
    "            \"\"\"\n",
    "            #Q[old_observation[0], action, old_observation[1]] = \\\n",
    "            #    Q[old_observation[0], action, old_observation[1]] + alpha * (\n",
    "            #        reward + gamma*criticQ[observation[0],\n",
    "            #                               np.argmax(Q[observation[0], :, observation[1]]),observation[1]] \n",
    "            #        - Q[old_observation[0], action, old_observation[1]])\n",
    "            \n",
    "\n",
    "            \n",
    "            delta = \\\n",
    "                 alpha * (\n",
    "                    reward + gamma*criticQ[observation[0]\n",
    "                                           ,observation[1],np.argmax(Q[observation[0], observation[1],:])] \n",
    "                    - Q[old_observation[0], old_observation[1],action])\n",
    "\n",
    "            Q += delta * eligibility\n",
    "            eligibility*=lamb\n",
    "            #eligibility1*=lamb\n",
    "            #eligibility2*=lamb\n",
    "            #calculate delta by using double Q update \n",
    "            #delta = reward + gamma*criticQ[observation[0],\n",
    "            #        observation[1],np.argmax(Q[observation[0], observation[1], :])] \n",
    "            #- Q[old_observation[0], old_observation[1], action]\n",
    "            #input(\"delta\")\n",
    "            #input(delta)\n",
    "            #update the Q \n",
    "            #Q += alpha * delta #* eligibility\n",
    "            #input(\"Q\")\n",
    "            #input(Q)\n",
    "            #decay the eligibility\n",
    "            #eligibility *= lamb\n",
    "\n",
    "            if done:#if the episode has terminated\n",
    "                l.append(total_score)#append the score in the episode to list\n",
    "                break\n",
    "\n",
    "    return np.array(l)#return the list of scores achieved in all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  n_step_double_Q_learning(env,num_episodes,Q1,Q2,alpha,epsilon,gamma,n):\n",
    "    \"\"\"\n",
    "    :param : env\n",
    "    :param : num_episodes \n",
    "    :param : Q1 - 2D array of values for state action pairs\n",
    "    :param : Q2 - 2D array of values for state action pairs\n",
    "    :param : alpha - learning rate\n",
    "    :param : epsilon - explore-exploit trade-off parameter\n",
    "    :param : gamma - discount factor\n",
    "    :param : lamb - eligibility trace decay factor\n",
    "    \"\"\"\n",
    "\n",
    "    l = [] #this list will store the scores achieved in all episodes\n",
    "    for _ in range(num_episodes):\n",
    "        episode_seq = []\n",
    "        total_score = 0#at the start of every new episode set total score to 0\n",
    "        observation = env.reset()# reset the environment to start\n",
    "        r_list = []\n",
    "        while(True):\n",
    "            R = np.random.randint(1,3)#random varibale to decide if to use Q1 or Q2\n",
    "            #input(\"R=\"+str(R))\n",
    "            if R==1: #if random variable R = 1 then choose Q1 to take action\n",
    "                Q=Q1\n",
    "                criticQ=Q2\n",
    "            else: \n",
    "                Q=Q2 #if random variable R = 2 then choose Q2 to take action\n",
    "                criticQ=Q1\n",
    "\n",
    "            Q_old = deepcopy(Q)#make a copy of Q\n",
    "            x = np.random.binomial(n=1,size=1,p=1-epsilon)#bernoulli random variable with p=1-epsilon\n",
    "            action = -1\n",
    "            if x[0] == 1:#exploitation\n",
    "                #choose action with highest avg value\n",
    "                #action = np.argmax(0.5*criticQ[observation[0],observation[1],:]+0.5*Q[observation[0],observation[1],:])\n",
    "                action = np.argmax(0.5*criticQ[observation[0],observation[1],:]+0.5*Q[observation[0],observation[1],:])\n",
    "            else:#exploration\n",
    "                action = np.random.randint(low=0,high=4)\n",
    "            \n",
    "            #before taking an action store the current state\n",
    "            old_observation = deepcopy(observation)\n",
    "            #take an action\n",
    "            observation, reward, done, info = env.step(action)#take action\n",
    "            #store in episode_seq\n",
    "            episode_seq.append({\"S\":old_observation,\"R\":reward,\"A\":action,\"Q1\":Q,\"Q2\":criticQ})\n",
    "            total_score += max(reward,0)#update the score\n",
    "            r_list.append(reward)\n",
    "            \"\"\"\n",
    "            Q-learning update step\n",
    "            \"\"\"\n",
    "\n",
    "            #calculate delta by using double Q update\n",
    "            #delta = reward + gamma*criticQ[observation[0]\n",
    "            #                               ,observation[1],np.argmax(Q[observation[0], observation[1],:])] \n",
    "            #- Q[old_observation[0], old_observation[1],action]\n",
    "\n",
    "            #Q += alpha * delta * eligibility\n",
    "            \n",
    "             \n",
    "            if done:#if the episode has terminated\n",
    "                l.append(total_score)#append the score in the episode to list\n",
    "                break\n",
    "        #now after the episode is ended update all the states\n",
    "        for i in range(len(episode_seq)):\n",
    "            G = 0\n",
    "            boot_Q2 = 0\n",
    "            s = episode_seq[i][\"S\"]\n",
    "            a = episode_seq[i][\"A\"]\n",
    "            q1 = episode_seq[i][\"Q1\"]\n",
    "            q2 = episode_seq[i][\"Q2\"]\n",
    "            #print(q1.shape,\" \",q2.shape)\n",
    "            for j in range(i,min(i+n,len(episode_seq))):\n",
    "                G += math.pow(gamma,j-i)*episode_seq[j][\"R\"]            \n",
    "            if i+n < len(episode_seq): #then bootstrap\n",
    "                n_s = episode_seq[i+n][\"S\"]\n",
    "                #print(n_s)\n",
    "                n_max_a = np.argmax(q1[n_s[0],n_s[1],:])\n",
    "                boot_Q2 = q2[n_s[0],n_s[1],n_max_a]\n",
    "            delta = (G + math.pow(gamma,n)*boot_Q2) - q1[s[0],s[1],a]\n",
    "            q1[s[0],s[1],a] += alpha*delta\n",
    "            \n",
    "            \n",
    "    return np.array(l)#return the list of scores achieved in all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runner for Double Q learning\n",
    "def run_doubleQ_learning(max_time_steps, max_lives_lost, \n",
    "                  alpha, epsilon, gamma, lamb, \n",
    "                  num_episdoes, num_decay_batches=5, decay_factor=0.999):\n",
    "    #create environment\n",
    "    env = EnvSetTarget(max_time_steps,max_lives_lost)\n",
    "    #initialize Q\n",
    "    Q1 = np.zeros((max_time_steps+1,max_lives_lost+1,len(env.action_space)))\n",
    "    Q2 = np.zeros((max_time_steps+1,max_lives_lost+1,len(env.action_space)))\n",
    "    #initialize var to keep list of running avg\n",
    "    sum = 0\n",
    "    l = []\n",
    "    for _ in range(num_decay_batches):\n",
    "        epsilon *= decay_factor\n",
    "        x=dup_double_Q_learning(env,num_episodes,Q1,Q2,alpha,epsilon,gamma,lamb)\n",
    "        for i in range(len(x)):\n",
    "            sum+=1/(len(l)+i+1) * (x[i]-sum)\n",
    "            l.append(sum)\n",
    "\n",
    "    return 0.5*Q1+0.5*Q2,np.array(l)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q1 = np.zeros((7,3,4))\n",
    "Q2 = np.zeros((7,3,4))\n",
    "env = EnvSetTarget(6,2)\n",
    "num_episodes = 5000\n",
    "gamma = 1\n",
    "lamb = 0\n",
    "alpha=0.2\n",
    "epsilon=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=n_step_double_Q_learning(env,num_episodes,Q1,Q2,alpha,epsilon,gamma,n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-1137a37e6630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdup_double_Q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mepsilon\u001b[0m\u001b[0;34m*=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-7d9e35a9788c>\u001b[0m in \u001b[0;36mdup_double_Q_learning\u001b[0;34m(env, num_episodes, Q1, Q2, alpha, epsilon, gamma, lamb)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m#choose action with highest avg value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;31m#action = np.argmax(0.5*criticQ[observation[0],observation[1],:]+0.5*Q[observation[0],observation[1],:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcriticQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#exploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    l=dup_double_Q_learning(env,num_episodes,Q1,Q2,alpha,epsilon,gamma,lamb)\n",
    "    epsilon*=0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q,l = run_doubleQ_learning(max_time_steps=6, max_lives_lost=2, \n",
    "                  alpha=0.3, epsilon=0.07, gamma=gamma, lamb=lamb, \n",
    "                  num_episdoes=num_episodes, num_decay_batches=5, decay_factor=0.999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 14, 18,  5, 18, 22, 18, 14, 24, 20,  5, 14, 22, 22, 22,  5, 22,\n",
       "       22,  9, 28,  6, 24, 22,  2, 13, 28, 17, 22, 16, 22])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[4000:4030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = np.zeros(Q1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 10.91427481,  11.97674714,  16.52970016,  11.91731218],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[ 10.18978307,   9.05491505,  14.90209113,  10.60435556],\n",
       "        [  7.74462816,   5.47246217,   3.75116899,   4.4264314 ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  8.57997622,   8.64655526,   9.05116115,  14.34453399],\n",
       "        [  5.98404837,   8.05467141,   6.16177814,   4.16932715],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  8.14161211,   7.62686135,   9.84031049,   7.579892  ],\n",
       "        [  4.48225343,   5.01806796,   9.12983222,   4.88208126],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  3.54907517,   5.13798141,   4.68307695,   7.39616085],\n",
       "        [  3.92123883,   4.25851313,   6.8872654 ,   3.94815687],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  0.86462215,   1.43053107,   2.92423964,   1.88084337],\n",
       "        [  0.91634365,   1.71736971,   2.31286823,   4.40362417],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q += (Q1+Q2)/2\n",
    "(Q1+Q2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 10.91427481,  11.97674714,  16.52970016,  11.91731218],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[ 10.18978307,   9.05491505,  14.90209113,  10.60435556],\n",
       "        [  7.74462816,   5.47246217,   3.75116899,   4.4264314 ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  8.57997622,   8.64655526,   9.05116115,  14.34453399],\n",
       "        [  5.98404837,   8.05467141,   6.16177814,   4.16932715],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  8.14161211,   7.62686135,   9.84031049,   7.579892  ],\n",
       "        [  4.48225343,   5.01806796,   9.12983222,   4.88208126],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  3.54907517,   5.13798141,   4.68307695,   7.39616085],\n",
       "        [  3.92123883,   4.25851313,   6.8872654 ,   3.94815687],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  0.86462215,   1.43053107,   2.92423964,   1.88084337],\n",
       "        [  0.91634365,   1.71736971,   2.31286823,   4.40362417],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.max(Q,axis=2)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "bellman_V = np.array([[ 17.8688 ,   0.     ,   0.     ],\n",
    "       [ 15.60576,  10.92096,   0.     ],\n",
    "       [ 13.0944 ,   9.6512 ,   0.     ],\n",
    "       [ 10.352  ,   8.064  ,   0.     ],\n",
    "       [  7.2    ,   6.08   ,   0.     ],\n",
    "       [  3.6    ,   3.6    ,   0.     ],\n",
    "       [  0.     ,   0.     ,   0.     ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96805169129801039"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sqrt(np.mean(np.power(bellman_V - V,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16.52970016,   0.        ,   0.        ],\n",
       "       [ 14.90209113,   7.74462816,   0.        ],\n",
       "       [ 14.34453399,   8.05467141,   0.        ],\n",
       "       [  9.84031049,   9.12983222,   0.        ],\n",
       "       [  7.39616085,   6.8872654 ,   0.        ],\n",
       "       [  2.92423964,   4.40362417,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
